{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os;\n",
    "import numpy as np;\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer;\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1\"#选择哪一块gpu,-1代表cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file <'NO'> was ignored for <txt> file only\n",
      "file <'Nothing.txt'> was ignored for its zero size\n",
      "done, while Getting 8 documents loaded\n"
     ]
    }
   ],
   "source": [
    "# 载入文档\n",
    "def load_documents(path):\n",
    "    files =[];\n",
    "    for filename in os.listdir(path):\n",
    "        path_of_file = path + '/' + filename;\n",
    "        if os.path.getsize(path_of_file) > 0:\n",
    "            if len(filename) > 3 and filename[-3:] == 'txt':\n",
    "                f = open(path_of_file,'r');\n",
    "                content = f.read();\n",
    "                files.append(content);\n",
    "                f.close();\n",
    "            else:\n",
    "                print('file <\\'' + filename + '\\'> was ignored for <txt> file only');\n",
    "        else:\n",
    "            print('file <\\'' + filename + '\\'> was ignored for its zero size');\n",
    "    print('done, while Getting {} documents loaded'.format(len(files)))\n",
    "    return files;\n",
    "# 文档目录，注意修改。\n",
    "path = '/home/pp21/data/document';\n",
    "document = load_documents(path);\n",
    "# '''2个名为 <No*> 的文件为验证上述函数泛化能力的空文件'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 载入停顿词\n",
    "path = '/home/pp21/data/stop_words/stop words.txt';\n",
    "f = open(path,'r',);\n",
    "stop_words = f.read();\n",
    "f.close();\n",
    "stop_words = stop_words.split('\\n');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 载入实体\n",
    "stop_words += ['sam','michelle','lele'];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 100;\n",
    "num_documents = len(document);\n",
    "num_topics = 4;\n",
    "tokenizer = Tokenizer(num_words=num_words);# 内容量为100的词典\n",
    "tokenizer.fit_on_texts(document);# 以频率次数排列词典\n",
    "word_index = tokenizer.word_index;\n",
    "words = list(word_index.keys());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(words,num_words,stop_words):\n",
    "    len_words = len(words);\n",
    "    t = 0;\n",
    "    n = 0;\n",
    "    selected_words = [];\n",
    "    while t < len_words and n < num_words:\n",
    "        if not words[t] in stop_words:\n",
    "            selected_words.append(words[t]);\n",
    "            n += 1;\n",
    "        t += 1;\n",
    "    if len(selected_words) < num_words:\n",
    "        print('quantities of selected words were not Enough; Get/Require: {}/{}'.format(len(selected_words),num_words));\n",
    "    return selected_words;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去掉停用词\n",
    "def remove_stopwords(words,num_words,stop_words):\n",
    "    len_words = len(words);\n",
    "    t = 0;\n",
    "    n = 0;\n",
    "    selected_words = [];\n",
    "    while t < len_words and (n < num_words or num_words < 0):\n",
    "        if not words[t] in stop_words:\n",
    "            selected_words.append(words[t]);\n",
    "            n += 1;\n",
    "        t += 1;\n",
    "    if len(selected_words) < num_words:\n",
    "        print('quantities of selected words were not Enough; Get/Require: {}/{}'.format(len(selected_words),num_words));\n",
    "    return selected_words;\n",
    "words = remove_stopwords(words,-1,stop_words);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取TF IDF TFIDF 词频 单词\n",
    "def get_TFIDF(texts,words,num_words=100,more_info=False,stop_words=None):\n",
    "    tokenizer = Tokenizer();# 内容量为100的词典\n",
    "    tokenizer.fit_on_texts(texts);# 以频率次数排列词典\n",
    "    word_index = tokenizer.word_index;\n",
    "    word_counts = tokenizer.word_counts;\n",
    "    TF = [];\n",
    "    F = [];\n",
    "    for text in texts:\n",
    "        this_tokenizer = Tokenizer();\n",
    "        this_tokenizer.fit_on_texts([text]);\n",
    "        this_word_counts = this_tokenizer.word_counts;\n",
    "        this_words = list(this_tokenizer.word_index.keys());\n",
    "        f = [];\n",
    "        t_frequence = [];\n",
    "        for word in words:\n",
    "            if word in this_words:\n",
    "                fre = this_word_counts[word];\n",
    "                t_fre = fre/len(this_words);\n",
    "                f.append(fre);\n",
    "                t_frequence.append(t_fre);\n",
    "            else:\n",
    "                f.append(0);\n",
    "                t_frequence.append(0);\n",
    "        TF.append(t_frequence);\n",
    "        F.append(f);\n",
    "    F = np.array(F);\n",
    "    TF = np.array(TF);\n",
    "    DF = [];\n",
    "    for word in words:\n",
    "        df = word_counts[word];\n",
    "        DF.append(df);\n",
    "    DF = np.array(DF).reshape(-1,1);\n",
    "    IDF = np.log10(len(texts)/(1+DF));\n",
    "    TFIDF = TF.T*IDF;\n",
    "    if more_info == True:\n",
    "        return F.T,TF.T,DF,TFIDF;\n",
    "    else:\n",
    "        return TFIDF;\n",
    "# 只需要TFIDF则考虑 more_info = False（默认）\n",
    "F,TF,DF,TFIDF = get_TFIDF(texts=document,words=words,more_info=True,stop_words=stop_words);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = len(words)\n",
    "# 初始化概率矩阵（概率分布）\n",
    "n_wd_matrix = F;\n",
    "p_zd_matrix = np.abs(np.random.randn(num_topics,num_documents));\n",
    "p_wz_matrix = np.abs(np.random.randn(num_words,num_topics));\n",
    "p_zwd_matrix = np.abs(np.random.randn(num_topics,num_words,num_documents));\n",
    "p_zd_matrix = p_zd_matrix/np.max(p_zd_matrix);\n",
    "p_wz_matrix = p_wz_matrix/np.max(p_wz_matrix);\n",
    "p_zwd_matrix = p_zwd_matrix/np.max(p_zwd_matrix);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tdelta*100: 1.96262449\tL: -2358.84860876\n",
      "2\tdelta*100: 3.14597136\tL: -2357.99073782\n",
      "3\tdelta*100: 4.88638969\tL: -2356.08631777\n",
      "4\tdelta*100: 7.17549710\tL: -2351.72516936\n",
      "5\tdelta*100: 9.36142925\tL: -2342.36491953\n",
      "6\tdelta*100: 9.36735092\tL: -2325.19191029\n",
      "7\tdelta*100: 5.06149498\tL: -2303.03017777\n",
      "8\tdelta*100: 2.65270696\tL: -2288.01304798\n",
      "9\tdelta*100: 1.80161945\tL: -2279.69457538\n",
      "10\tdelta*100: 0.74098702\tL: -2273.28591674\n"
     ]
    }
   ],
   "source": [
    "# 矩阵运算\n",
    "import copy;\n",
    "epochs = 10;\n",
    "for e in range(epochs):\n",
    "    last_zd_m = copy.deepcopy(p_zd_matrix);\n",
    "    '''那个sb玩意儿搞得浅copy？？？？？我特么缺你这点内存吗？？？？'''\n",
    "    # E-step\n",
    "    for k in range(num_topics):\n",
    "        p_zwd_matrix[k,:,:] = np.dot(p_wz_matrix[:,k:k+1],p_zd_matrix[k:k+1,:])/np.dot(p_wz_matrix,p_zd_matrix);\n",
    "    L = np.sum(n_wd_matrix*np.sum(p_zwd_matrix*np.log(np.dot(p_wz_matrix,p_zd_matrix)),axis=0));\n",
    "    # M-step\n",
    "    for i in range(num_documents):\n",
    "        p_zk = np.dot(p_zwd_matrix[:,:,i],n_wd_matrix[:,i]);\n",
    "        p_zd_matrix[:,i] = p_zk/np.sum(p_zk);\n",
    "    p_wj = [];\n",
    "    for j in range(num_words):\n",
    "        res = np.dot(p_zwd_matrix[:,j,:],n_wd_matrix[j,:])\n",
    "        p_wj.append(res);\n",
    "    p_wj = np.array(p_wj);\n",
    "    p_wz_matrix = p_wj/np.sum(p_wj,axis=0);\n",
    "    delta = np.max(np.abs(last_zd_m-p_zd_matrix));# 迭代中绝对值最大误差（无穷范数）\n",
    "    print('{:d}\\tdelta*100: {:.8f}\\tL: {:.8f}'.format(e+1,delta*100,L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic of documents:\n",
      "document\\topic\t 1 \t 2 \t 3 \t 4 \t\n",
      "                --------------------------------\n",
      "\t1    |\t 0\t 0\t 0\t 1.0000\t\n",
      "\t2    |\t 0.9994\t 0\t 0.0006\t 0\t\n",
      "\t3    |\t 0.0002\t 0\t 0\t 0.9998\t\n",
      "\t4    |\t 1.0000\t 0\t 0\t 0\t\n",
      "\t5    |\t 0\t 1.0000\t 0\t 0\t\n",
      "\t6    |\t 0.9998\t 0\t 0.0002\t 0\t\n",
      "\t7    |\t 0\t 1.0000\t 0\t 0\t\n",
      "\t8    |\t 0\t 0\t 1.0000\t 0\t\n"
     ]
    }
   ],
   "source": [
    "# 打印文档/主题概率分布\n",
    "def print_topic(m):\n",
    "    print('topic of documents:');\n",
    "    print('document\\\\topic\\t',end='')\n",
    "    for i in range(m.shape[0]):\n",
    "        print(' {} \\t'.format(i+1),end='');\n",
    "        pass;\n",
    "    print('',end='\\n');\n",
    "    print(' '*8*2,end='');\n",
    "    for i in range(m.shape[0]):\n",
    "        print('-'*8,end='');\n",
    "        pass;\n",
    "    print('',end='\\n');\n",
    "    for i in range(m.shape[1]):\n",
    "        print('\\t{}    |\\t'.format(i+1),end='')\n",
    "        for j in range(m.shape[0]):\n",
    "            if m[j,i] < 1e-4:\n",
    "                print(' 0\\t',end='')\n",
    "            else:\n",
    "                print(' {:.4f}\\t'.format(m[j,i]),end='');\n",
    "        print('',end='\\n');\n",
    "print_topic(p_zd_matrix);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words of topic:\n",
      "topic\\words\t 1 \t\t 2 \t\t 3 \t\t 4 \t\t 5 \t\t 6 \t\t\n",
      "\t1:\tclimb\t\tmountain\tenergy\t\tfish\t\tmeant\t\tmountains\t\n",
      "\t2:\tearthquake\tcongratulations\ttv\t\ttwins\t\tnurse\t\tdamage\t\t\n",
      "\t3:\tair\t\tlive\t\tmountain\tfeel\t\tfront\t\tclimb\t\t\n",
      "\t4:\tphoenix\t\tborn\t\tbacteria\tbabys\t\tflies\t\tpeople\t\t\n"
     ]
    }
   ],
   "source": [
    "# 打印单词/主题概率分布\n",
    "def print_words_of_topic(m,n_words,words):\n",
    "    print('words of topic:');\n",
    "    print('topic\\\\words\\t',end='')\n",
    "    for i in range(min(m.shape[0],n_words)):\n",
    "        print(' {} \\t\\t'.format(i+1),end='');\n",
    "        pass;\n",
    "    print('',end='\\n');\n",
    "    for i in range(m.shape[1]):\n",
    "        m_i = list(m[:,i]);\n",
    "        sorted_nums = sorted(enumerate(m_i), key=lambda x: x[1],reverse=True)\n",
    "        idx = [j[0] for j in sorted_nums]\n",
    "        nums = [j[1] for j in sorted_nums]\n",
    "        print('\\t{}:\\t'.format(i+1),end='')\n",
    "        for j in range(min(m.shape[0],n_words)):\n",
    "            word = words[idx[j]];\n",
    "            print(word,end='');\n",
    "            if len(word)<8:\n",
    "                print('\\t\\t',end='');\n",
    "            else:\n",
    "                print('\\t',end='');\n",
    "        print('',end='\\n');\n",
    "print_words_of_topic(m=p_wz_matrix,n_words=6,words=words);\n",
    "# n_words: 最大单词显示数量\n",
    "# 该函数依概率打印前 n_words 个单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08171369215249193"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(p_wz_matrix[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.089424948047765"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(p_wz_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d5d21e79f1d5ba1c56103bc8e531dbdd1e64dc8cd604e6393178b5ef12db8e16"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('tf_gpu': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
